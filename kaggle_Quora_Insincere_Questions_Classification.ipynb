{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# <center>文本分类任务的基本流程<center>\n## <center>以kaggle \"Quora Insincere Questions Classification\" 为例<center>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## 1、数据探索&数据清洗\n\n- 对于分类任务，首先需要知道是几分类，类别之间的比例分布是怎样的……\n- 数据清洗方面，缺失值填充，针对英文特别处理标点符号，大小写的转换，数字的处理，"},{"metadata":{"trusted":true,"_uuid":"aa68a86f59f51ce1df4049263aa3d079c1f32240"},"cell_type":"code","source":"import os\nimport time\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d9403660c25086be541982f46078759c782a40a"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb903c680d1f30de9b015fa14ee73cad384a0ec6"},"cell_type":"code","source":"# train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1693e46846172b7463f2f170325d93c460b2704"},"cell_type":"code","source":"# train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e2d920e08d9895a0475957bdc595a491a5c34e6"},"cell_type":"code","source":"# train_df['target'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90bf05701edcfbb52eecfc23925c8d9f13aa2a6f"},"cell_type":"code","source":"train_df.loc[train_df['target']==1,'question_text'][:5].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"532862d16fe735c5f20aeddb821830bc2b5327ce"},"cell_type":"code","source":"# train_df = train_df.sample(frac=0.5)\ntrain_df[\"question_text\"].fillna('__na__',inplace=True)\ntest_df[\"question_text\"].fillna('__na__',inplace=True)\n\ntrain_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\ntest_df[\"question_text\"] = test_df[\"question_text\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3be70a4c3eb34779c55a897e091d574bbf368096"},"cell_type":"markdown","source":"### 数据清洗的一些细节\n\n- 处理重复的某些标点符号，“!，?，.”\n\n比如将“!!”转换成“multiExclamation”，“??”转换成“multiQuestion”，“..”转换成“multiStop”。\n\nExample: How do I overcome the fear of facing an interview? It's killing me inside..what should I do? -> How do I overcome the fear of facing an interview? It's killing me inside multiStop what should I do?\n\n- 将标点符号和单词用空格隔开，便于分词\n\n- 处理英文缩写\n\nExample: What's the scariest thing that ever happened to anyone? -> What is the scariest thing that ever happened to anyone?\n\n- 去除停用词\n\n- 词形还原\n\nExample: How do modern military submarines reduce noise to achieve stealth? -> how do modern militari submarin reduc nois to achiev stealth ?\n    "},{"metadata":{"trusted":true,"_uuid":"79ac5edfbb7186256597938875356c4d71e4f249"},"cell_type":"code","source":"import re\nfrom nltk import tokenize\nfrom nltk.corpus import stopwords\nstoplist = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n     '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n     '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n     '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n     '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ncontraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), \n                        (r'(\\w+)n\\'t', '\\g<1> not'),(r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), \n                        (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n\ndef clean_text(text):\n    text = re.sub('[0-9]+','',text)\n    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n    \n    for punct in puncts:\n        text = text.replace(punct, f' {punct} ')\n    \n    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n    for (pattern, repl) in patterns:\n        (text, count) = re.subn(pattern, repl, text)\n    \n    text_split = tokenize.word_tokenize(text)\n    text = [word for word in text_split if word not in stoplist]\n    text = [wnl.lemmatize(word) for word in text]\n    \n    return \" \".join(text)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc026344c6b9039acfd01df819c60fa3253e4ff0"},"cell_type":"code","source":"train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe480507b8704a1408b2f8035af3640465bb1604"},"cell_type":"code","source":"# train_df[\"text_len\"] = train_df[\"question_text\"].map(lambda x: len(x.split()))\n# train_df[\"text_len\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aae632749a919392b2ab46f0a6e873cdce353c12"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.05, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a9e38c97dcdd265d4b3b5738ba3935c5b7db6f9"},"cell_type":"code","source":"train_X = train_df[\"question_text\"].values\nval_X = val_df[\"question_text\"].values\ntest_X = test_df[\"question_text\"].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29c834af7e7582aceab42b88d862446383ea0cf1"},"cell_type":"markdown","source":"### 将文本数据序列化，数字化，以适合用深度学习模型来训练"},{"metadata":{"trusted":true,"_uuid":"6dafd691e4f134bf1dd6fe694377630dc4fddcf0"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 70 # max number of words in a question to use\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0acc4815aa53822dc39dd2bfce7770fb83106085"},"cell_type":"markdown","source":"### 打乱数据"},{"metadata":{"trusted":true,"_uuid":"ace3ffaa8a4cb4ebada995f78fabd88497463524"},"cell_type":"code","source":"## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values  \n\n#shuffling the data\nnp.random.seed(2018)\ntrn_idx = np.random.permutation(len(train_X))\nval_idx = np.random.permutation(len(val_X))\n\ntrain_X = train_X[trn_idx]\nval_X = val_X[val_idx]\ntrain_y = train_y[trn_idx]\nval_y = val_y[val_idx]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed12116b6ec5188483e59bc45e3d4c69a727d298"},"cell_type":"markdown","source":"## 2、加载词向量\n\n    我们使用提供的预训练词向量，官方给了四种不同的词向量文件，我们选择 `glove.840B.300d.txt` 和 `paragram_300_sl999.txt。对每个词我们取这两者词向量的均值向量作为该词的最终词向量。"},{"metadata":{"trusted":true,"_uuid":"7aa8a737fd86d94216d55e14309e7e153baf7e0a"},"cell_type":"code","source":"print(os.listdir(\"../input/embeddings\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d45d283c1257dc1f91da66f4af24399c82f4e18b"},"cell_type":"code","source":"def load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n\ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.0053247833,0.49346462\n    embed_size = all_embs.shape[1]\n    print(emb_mean,emb_std,\"para\")\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a052f224d0808ebd27f2b3d2a0d6066eeefc6551"},"cell_type":"code","source":"word_index = tokenizer.word_index\nembedding_matrix_1 = load_glove(word_index)\nembedding_matrix_3 = load_para(word_index)\nembedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42506eb344784d47af1ff469326c87cf5a534ab2"},"cell_type":"code","source":"import gc\ndel embedding_matrix_1, embedding_matrix_3\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b666b0ef9c2c6acaab3a61bdbd9667e174dee5d2"},"cell_type":"markdown","source":"## 万事俱备，只欠东风\n\n## 3、初级篇--使用CNN\\LSTM\\CNN+LSTM来做文本分类"},{"metadata":{"_uuid":"af428f2b9ebae0f50c5905218e6d21de3f0f0d3f"},"cell_type":"markdown","source":"**我们首先定义一个训练模型的函数和评估模型的函数**"},{"metadata":{"trusted":true,"_uuid":"ec25f5399dcbeb3f962a33836391ce3ec7a528cd"},"cell_type":"code","source":"from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, MaxPooling1D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7648d52b880521a7a59599e3a05f4f85689c5810"},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef train_pred(model, epochs=2):\n    for e in range(epochs):\n        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n        search_result = threshold_search(val_y, pred_val_y)\n        print(search_result)\n    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n    return pred_val_y, pred_test_y\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.001 for i in range(250,450)]:\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fde2198c432fe0543aa972ef41beacc9463b8b8"},"cell_type":"code","source":"# 定义CNN模型\ndef model_cnn(embedding_matrix):\n    filter_sizes = [1,2,3,5]\n    num_filters = 36\n\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = Reshape((maxlen, embed_size, 1))(x)\n\n    maxpool_pool = []\n    for i in range(len(filter_sizes)):\n        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                     kernel_initializer='he_normal', activation='elu')(x)\n        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\n    z = Concatenate(axis=1)(maxpool_pool)   \n    z = Flatten()(z)\n    z = Dropout(0.1)(z)\n\n    outp = Dense(1, activation=\"sigmoid\")(z)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9c50fc84b0dc7467572c84281b16fd28b989510"},"cell_type":"code","source":"pred_val_y1, pred_test_y1 = train_pred(model_cnn(embedding_matrix), epochs=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4a56fbff736722d4a92e6a1fad478efe4fa75c4"},"cell_type":"code","source":"def f1(y_true, y_pred):\n    '''\n    metric from here \n    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n    '''\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n        Only computes a batch-wise average of precision.\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef model_lstm(embedding_matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.1)(x)\n    x = Bidirectional(CuDNNLSTM(50, return_sequences=False))(x)\n    x = Dense(16, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)    \n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"276273245c58ad44a87038d9934c23e0911b4100"},"cell_type":"code","source":"pred_val_y2, pred_test_y2 = train_pred(model_lstm(embedding_matrix), epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be5ec03a2d313517c56dda147f93a045fd4ec097"},"cell_type":"code","source":"def model_cnn_lstm(embedding_matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.1)(x)\n    x = Conv1D(64,3,activation='relu')(x)\n    x = MaxPooling1D(2)(x)\n    x = Bidirectional(CuDNNLSTM(50, return_sequences=False))(x)\n    x = Dense(16, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d71334fe026b78fb18a61561abf67fdfe07836e9"},"cell_type":"code","source":"pred_val_y3, pred_test_y3 = train_pred(model_cnn_lstm(embedding_matrix), epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8dad9156648476b3affc45adf1eb9adab932596"},"cell_type":"markdown","source":"## 4、进阶篇--为模型添加魔法\n\n**使用Attention\\Capsule等网络增强模型的特征提取能力，使用CLR学习率策略让模型更快收敛并获得更好的性能。**\n\n**模型融合**"},{"metadata":{"_uuid":"731911b6265e7669561dde62a5401de1ab74b7ba"},"cell_type":"markdown","source":"### Attention\n\n注意力机制的直觉是：人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。\n\n$$Attention(Query, Source)=\\sum _{i=1}^{L_x} Similarity(Query,key_i)*Value_i $$\n\n- soft attention $C_i = \\sum _{j=1}^{L_x} \\alpha _{ij} h_j$\n- self attention\n- multi-head attention"},{"metadata":{"trusted":true,"_uuid":"bd4b02737c2fda960c32d0f83634cf4b80bf7713"},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68f025d64420861d7825aa68e5035b968abc2c98"},"cell_type":"code","source":"def model_lstm_atten(embedding_matrix):\n    \n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.1)(x)\n    x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x)\n    y = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n    \n    atten_1 = Attention(maxlen)(x)\n    atten_2 = Attention(maxlen)(y)\n    avg_pool = GlobalAveragePooling1D()(y)\n    max_pool = GlobalMaxPooling1D()(y)\n    \n    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n    conc = Dense(16, activation=\"relu\")(conc)\n    conc = Dropout(0.1)(conc)\n    outp = Dense(1, activation=\"sigmoid\")(conc)    \n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cd81465ab6c9b99a90d84e6eb13953ef58121b1"},"cell_type":"code","source":"pred_val_y4, pred_test_y4 = train_pred(model_lstm_atten(embedding_matrix), epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3db371cf0c5daf9955b18ea19b40dab08ef24bcc"},"cell_type":"markdown","source":"## Capsule胶囊网络\n\n所谓胶囊，就是一个向量，它可包含任意个值，每个值代表了当前需要识别的物体（比如图片）的一个特征。相比传统CNN的卷积层的每个值都是上一层某一块区域和卷积核完成卷积操作，即线性加权求和的结果，它只有一个值，是标量。而胶囊网络的每一个值都是向量，也就是说，这个向量不仅可表示物体的特征、还可以包括物体的方向、状态等等。\n"},{"metadata":{"trusted":true,"_uuid":"c16981ab374be12308a3a9ad5c581077b4ab5ed7"},"cell_type":"code","source":"def squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cf63077ba135cecdbfadf7f0740f9aa48dcaf0a"},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.layers import BatchNormalization\ndef model_capsule(embedding_matrix):\n    K.clear_session()\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(rate=0.2)(x)\n    x = Bidirectional(CuDNNGRU(100, return_sequences=True, \n                               kernel_initializer=initializers.glorot_normal(seed=12300),\n                               recurrent_initializer=initializers.orthogonal(gain=1.0, seed=10000)))(x)\n\n    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n    x = Flatten()(x)\n\n    x = Dense(100, activation=\"relu\", kernel_initializer=initializers.glorot_normal(seed=12300))(x)\n    x = Dropout(0.12)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(),)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05dd58977ed2e96fe5fcf8922be888ac432b7a0e"},"cell_type":"code","source":"pred_val_y5, pred_test_y5 = train_pred(model_capsule(embedding_matrix), epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab76b51c6a67986c31e88e86afa265dd24aa30aa"},"cell_type":"markdown","source":"## CLR学习率策略"},{"metadata":{"trusted":true,"_uuid":"a51f6833a79f35f1d10beebac93499f93adba8ea"},"cell_type":"code","source":"class CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8d10cb6f7b7136cc050aac7db860793a920cba4"},"cell_type":"code","source":"clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n               step_size=300., mode='exp_range',\n               gamma=0.99994)\ndef train_pred2(model, epochs=2):\n    for e in range(epochs):\n        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y), callbacks=[clr])\n        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n        search_result = threshold_search(val_y, pred_val_y)\n        print(search_result)\n    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n    return pred_val_y, pred_test_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21a2847df2f996555b42e9bea232bc91da9d53b6"},"cell_type":"code","source":"def model_3gru_atten(embedding_matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(100, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n    x = Attention(maxlen)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d269ff7d676ad6a18eb55f7fa67a76faece77777"},"cell_type":"code","source":"pred_val_y6, pred_test_y6 = train_pred2(model_3gru_atten(embedding_matrix), epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"728367114f87a772d55a0581ba77c385aa4c694d"},"cell_type":"code","source":"pred_val_y7, pred_test_y7 = train_pred2(model_lstm_atten(embedding_matrix), epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e43be0392a20bd3f9f492f3835a7f4508f584b7"},"cell_type":"markdown","source":"## 融合\n\n- 不同模型使用一样的词向量，同一个模型使用不同的词向量（多模型）\n- 简单加权融合\n- stacking\n- 其他"},{"metadata":{"trusted":true,"_uuid":"c8f878b245b1fd27a3778d3aedbf2eb931902c4e"},"cell_type":"code","source":"pred_val_y = 0.09*pred_val_y1 + 0.15*pred_val_y2 + 0.08*pred_val_y3 + 0.22*pred_val_y4 + \\\n            0.18*pred_val_y5 + 0.16*pred_val_y7 + 0.12*pred_val_y7\nsearch_result = threshold_search(val_y, pred_val_y)\nprint(search_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3ff4501af04c7a9fc4c882b672a0cb1d0e8e12b"},"cell_type":"code","source":"pred_test_y = 0.09*pred_test_y1 + 0.15*pred_test_y2 + 0.08*pred_test_y3 + 0.22*pred_test_y4 + \\\n            0.18*pred_test_y5 + 0.16*pred_test_y6 + 0.12*pred_test_y7\npred_test_y = (pred_test_y > search_result['threshold']).astype(int)\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['prediction'] = pred_test_y\nsub.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}